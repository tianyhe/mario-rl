{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gymnasium Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tianyhe/miniconda3/envs/mario-rl/lib/python3.11/site-packages/IPython/core/pylabtools.py:77: DeprecationWarning: backends is deprecated since IPython 8.24, backends are managed in matplotlib and can be externally registered.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torchvision import transforms as T\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from collections import deque\n",
    "import random, datetime, os, sys\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A tool kit for RL\n",
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensordict import TensorDict\n",
    "from torchrl.data import TensorDictReplayBuffer, LazyMemmapStorage\n",
    "\n",
    "# NES Emulator\n",
    "from nes_py.wrappers import JoypadSpace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the src directory to the path\n",
    "sys.path.append('../src')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the Mario environment\n",
    "env = gym.make(\"ALE/MarioBros-v5\", render_mode=\"rgb_array\")\n",
    "state = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action space: Discrete(18)\n",
      "Observation space: Box(0, 255, (210, 160, 3), uint8)\n"
     ]
    }
   ],
   "source": [
    "# Print environment information\n",
    "print(f\"Action space: {env.action_space}\")\n",
    "print(f\"Observation space: {env.observation_space}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Next state: (210, 160, 3)\n",
      "Reward: 0.0\n",
      "Done?: False\n",
      "Info: {'lives': 5, 'episode_frame_number': 4, 'frame_number': 4}\n"
     ]
    }
   ],
   "source": [
    "# Render the environment\n",
    "next_state, reward, done, trunc, info = env.step(action=0)\n",
    "print(f\"Next state: {next_state.shape}\")\n",
    "print(f\"Reward: {reward}\")\n",
    "print(f\"Done?: {done}\")\n",
    "print(f\"Info: {info}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the game of Mario, color information is not important. We can convert the image to grayscale to reduce the dimensionality of the observation space.\n",
    "\n",
    "We use Wrappers to preprocess the data before feeding it to the agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipFrame(gym.Wrapper):\n",
    "    def __init__(self, env, skip):\n",
    "        \"\"\"\n",
    "        Initialize the SkipFrame wrapper.\n",
    "        \n",
    "        Parameters:\n",
    "        env (gym.Env): The environment to wrap.\n",
    "        skip (int): The number of frames to skip.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        Repeat the action for the specified number of frames and sum the reward.\n",
    "\n",
    "        Parameters:\n",
    "        action: The action to perform on the environment.\n",
    "\n",
    "        Returns:\n",
    "        obs: The observation after performing the action.\n",
    "        total_reward: The sum of rewards obtained from repeated actions.\n",
    "        done: Whether the episode has ended.\n",
    "        trunk: Additional information (typically 'truncated' status in Gym).\n",
    "        info: Additional info from the environment.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "\n",
    "class GrayScaleObservation(gym.ObservationWrapper):\n",
    "    def __init__(self, env):\n",
    "        \"\"\"\n",
    "        Initialize the GrayScaleObservation wrapper.\n",
    "        \n",
    "        Parameters:\n",
    "        env (gym.Env): The environment to wrap.\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    def permute_orientation(self, observation):\n",
    "        \"\"\"\n",
    "        Permute the observation array from [H, W, C] to [C, H, W].\n",
    "        \n",
    "        Parameters:\n",
    "        observation (numpy.ndarray): The original observation array.\n",
    "        \n",
    "        Returns:\n",
    "        observation (torch.Tensor): The permuted observation tensor.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def observation(self, observation):\n",
    "        \"\"\"\n",
    "        Convert the observation to grayscale.\n",
    "        \n",
    "        Parameters:\n",
    "        observation (numpy.ndarray): The original observation array.\n",
    "        \n",
    "        Returns:\n",
    "        observation (torch.Tensor): The grayscale observation tensor.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "\n",
    "class ResizeObservation(gym.ObservationWrapper):\n",
    "    def __init__(self, env, shape):\n",
    "        \"\"\"\n",
    "        Initialize the ResizeObservation wrapper.\n",
    "        \n",
    "        Parameters:\n",
    "        env (gym.Env): The environment to wrap.\n",
    "        shape (int or tuple): The desired shape for the observation.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "\n",
    "    def observation(self, observation):\n",
    "        \"\"\"\n",
    "        Resize the observation to the specified shape.\n",
    "        \n",
    "        Parameters:\n",
    "        observation (numpy.ndarray): The original observation array.\n",
    "        \n",
    "        Returns:\n",
    "        observation (torch.Tensor): The resized observation tensor.\n",
    "        \"\"\"\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After applying the above wrappers to the environment, the final wrapped state consists of 4 gray-scaled consecutive frames stacked together, as shown above in the image on the left. Each time Mario makes an action, the environment responds with a state of this structure. The structure is represented by a 3-D array of size [4, 84, 84]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation space after preprocessing: Box(0, 255, (4, 84, 84), uint8)\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium.wrappers import FrameStack\n",
    "\n",
    "# import wrappers from ../src/env/wrappers.py\n",
    "from env.wrappers import SkipFrame, GrayScaleObservation, ResizeObservation\n",
    "\n",
    "# Apply Wrappers to environment\n",
    "env = SkipFrame(env, skip=4)\n",
    "env = GrayScaleObservation(env)\n",
    "env = ResizeObservation(env, shape=84)\n",
    "\n",
    "env = FrameStack(env, num_stack=4)\n",
    "\n",
    "# Print the observation space\n",
    "print(f\"Observation space after preprocessing: {env.observation_space}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a class Mario to represent our agent in the game. Mario should be able to:\n",
    "\n",
    "- Act according to the optimal action policy based on the current state (of the environment).\n",
    "\n",
    "- Remember experiences. Experience = (current state, current action, reward, next state). Mario caches and later recalls his experiences to update his action policy.\n",
    "\n",
    "- Learn a better action policy over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mario:\n",
    "    def __init__():\n",
    "        pass\n",
    "\n",
    "    def act(self, state):\n",
    "        \"\"\"Given a state, choose an epsilon-greedy action\"\"\"\n",
    "        pass\n",
    "\n",
    "    def cache(self, experience):\n",
    "        \"\"\"Add the experience to memory\"\"\"\n",
    "        pass\n",
    "\n",
    "    def recall(self):\n",
    "        \"\"\"Sample experiences from memory\"\"\"\n",
    "        pass\n",
    "\n",
    "    def learn(self):\n",
    "        \"\"\"Update online action value (Q) function with a batch of experiences\"\"\"\n",
    "        pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mario-rl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
